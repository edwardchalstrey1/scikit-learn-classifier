{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking for data science algorithms workflow: Example #1\n",
    "====\n",
    "\n",
    "**Algorithm:** Support Vector Classification of MNIST digit images.\n",
    "\n",
    "**Benchmarks:** Compare training time, prediction time and performance of the classifier on the local machine vs a Docker container on the local machine and compare different versions of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before getting started\n",
    "\n",
    "1) [Install Docker](https://docs.docker.com/v17.09/engine/installation/) on all machines required for benchmarking and set up your account on [Docker Hub](https://hub.docker.com). In this example, we are just using the local machine. \n",
    "\n",
    "2) Make sure the software (data science algorithm) you are developing is version controlled so you can push newer versions to GitHub\n",
    "\n",
    "3) Your algorithm/code should output benchmark data that can can be collected when it is run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run our algorithm locally and collect benchmark results\n",
    "---\n",
    "\n",
    "Version 1.0 and version 1.1 of the code use different classifier models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile classifier.py\n",
    "from sklearn import datasets, svm, metrics\n",
    "import time\n",
    "\n",
    "def results():\n",
    "\n",
    "    digits = datasets.load_digits()\n",
    "\n",
    "    n_samples = len(digits.images)\n",
    "    data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "    expected = digits.target[n_samples // 2:]\n",
    "\n",
    "    models = [svm.SVC(gamma=0.01),\n",
    "          svm.SVC(gamma=0.001)]\n",
    "\n",
    "    ### Version 1.0 ###\n",
    "\n",
    "    classifier = models[0]\n",
    "\n",
    "    ### Version 1.1 ###\n",
    "\n",
    "#     classifier = models[1]\n",
    "\n",
    "    start = time.time()\n",
    "    classifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])\n",
    "    end = time.time()\n",
    "    training_time = end - start\n",
    "\n",
    "    start = time.time()\n",
    "    predicted = classifier.predict(data[n_samples // 2:])\n",
    "    end = time.time()\n",
    "    classifier_time = end - start\n",
    "\n",
    "    report = metrics.classification_report(expected, predicted, output_dict=True)\n",
    "\n",
    "    performance = report['micro avg']['f1-score']\n",
    "\n",
    "    return([metrics.classification_report(expected, predicted), {\"Training time (s)\": training_time, \"Prediction time (s)\": classifier_time,\n",
    "    \"Performance (micro avg f1 score)\": report['micro avg']['f1-score']}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.65      0.79        88\n",
      "           1       1.00      0.74      0.85        91\n",
      "           2       1.00      0.64      0.78        86\n",
      "           3       1.00      0.64      0.78        91\n",
      "           4       1.00      0.55      0.71        92\n",
      "           5       0.93      0.98      0.95        91\n",
      "           6       1.00      0.68      0.81        91\n",
      "           7       1.00      0.49      0.66        89\n",
      "           8       0.25      1.00      0.40        88\n",
      "           9       1.00      0.61      0.76        92\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       899\n",
      "   macro avg       0.92      0.70      0.75       899\n",
      "weighted avg       0.92      0.70      0.75       899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import classifier\n",
    "local_report, local_results = classifier.results()\n",
    "print(local_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get median benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classifier\n",
    "import statistics as st\n",
    "\n",
    "repeats = 10\n",
    "tt = []\n",
    "pt = []\n",
    "t = []\n",
    "\n",
    "for i in range(0,repeats):\n",
    "    \n",
    "    report, results = classifier.results()\n",
    "    tt.append(results[\"Training time (s)\"])\n",
    "    pt.append(results[\"Prediction time (s)\"])\n",
    "    t.append(results[\"Performance (micro avg f1 score)\"])\n",
    "    \n",
    "local_results = {\"Training time (s)\": st.median(tt), \"Prediction time (s)\": st.median(pt),\n",
    "    \"Performance (micro avg f1 score)\": st.median(t)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Docker image for our algorithm and push to Docker Hub\n",
    "---\n",
    "\n",
    "1) Create a docker image for installing your software on a linux distribution with the bare essential dependencies and outputting the benchmark stats. Example Dockerfile [here](https://github.com/edwardchalstrey1/benchmarking_test/blob/master/classifier/Dockerfile).\n",
    "\n",
    "2) Build a docker container and tag the version as latest. Optionally also tag a version: ```docker build -t edwardchalstrey/classifier:latest -t edwardchalstrey/classifier:1.0 .```\n",
    "\n",
    "3) Push to Docker Hub. This allows you to then pull the container to any machine you wish to benchmark on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3\n",
    "\n",
    "RUN apt-get update\n",
    "RUN pip3 install numpy\n",
    "RUN pip3 install scipy\n",
    "RUN pip3 install scikit-learn\n",
    "\n",
    "COPY classifier.py /classifier.py\n",
    "COPY iterate_benchmarks.py /iterate_benchmarks.py\n",
    "\n",
    "CMD python3 iterate_benchmarks.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script to run benchmarks within Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing iterate_benchmarks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile iterate_benchmarks.py \n",
    "import classifier\n",
    "import statistics as st\n",
    "\n",
    "repeats = 10\n",
    "tt = []\n",
    "pt = []\n",
    "t = []\n",
    "\n",
    "for i in range(0,repeats):\n",
    "    \n",
    "    report, results = classifier.results()\n",
    "    tt.append(results[\"Training time (s)\"])\n",
    "    pt.append(results[\"Prediction time (s)\"])\n",
    "    t.append(results[\"Performance (micro avg f1 score)\"])\n",
    "    \n",
    "results = {\"Training time (s)\": st.median(tt), \"Prediction time (s)\": st.median(pt),\n",
    "    \"Performance (micro avg f1 score)\": st.median(t)}\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  105.5kB\r",
      "\r\n",
      "Step 1/8 : FROM python:3\n",
      " ---> ac069ebfe1e1\n",
      "Step 2/8 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 5a84d23aa7b5\n",
      "Step 3/8 : RUN pip3 install numpy\n",
      " ---> Using cache\n",
      " ---> 4383ac463a3b\n",
      "Step 4/8 : RUN pip3 install scipy\n",
      " ---> Using cache\n",
      " ---> 6fa2c9da864b\n",
      "Step 5/8 : RUN pip3 install scikit-learn\n",
      " ---> Using cache\n",
      " ---> 0b888dbaed11\n",
      "Step 6/8 : COPY classifier.py /classifier.py\n",
      " ---> Using cache\n",
      " ---> 332880ae4763\n",
      "Step 7/8 : COPY iterate_benchmarks.py /iterate_benchmarks.py\n",
      " ---> Using cache\n",
      " ---> cdbcdd0db4ae\n",
      "Step 8/8 : CMD python3 iterate_benchmarks.py\n",
      " ---> Using cache\n",
      " ---> 7175da4a27c8\n",
      "Successfully built 7175da4a27c8\n",
      "Successfully tagged edwardchalstrey/classifier:1.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker build -t edwardchalstrey/classifier:1.0 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [docker.io/edwardchalstrey/classifier]\n",
      "a4b781a8bd5f: Preparing\n",
      "9f74b5180e0a: Preparing\n",
      "f12d5b3c5c82: Preparing\n",
      "337d3babfd9c: Preparing\n",
      "493622b04a5f: Preparing\n",
      "65ef2276d16f: Preparing\n",
      "4b381ae03f9a: Preparing\n",
      "08a5b66845ac: Preparing\n",
      "88a85bcf8170: Preparing\n",
      "65860ac81ef4: Preparing\n",
      "a22a5ac18042: Preparing\n",
      "6257fa9f9597: Preparing\n",
      "578414b395b9: Preparing\n",
      "abc3250a6c7f: Preparing\n",
      "13d5529fd232: Preparing\n",
      "65ef2276d16f: Waiting\n",
      "4b381ae03f9a: Waiting\n",
      "6257fa9f9597: Waiting\n",
      "578414b395b9: Waiting\n",
      "08a5b66845ac: Waiting\n",
      "abc3250a6c7f: Waiting\n",
      "13d5529fd232: Waiting\n",
      "88a85bcf8170: Waiting\n",
      "65860ac81ef4: Waiting\n",
      "a22a5ac18042: Waiting\n",
      "a4b781a8bd5f: Layer already exists\n",
      "9f74b5180e0a: Layer already exists\n",
      "337d3babfd9c: Layer already exists\n",
      "f12d5b3c5c82: Layer already exists\n",
      "493622b04a5f: Layer already exists\n",
      "4b381ae03f9a: Layer already exists\n",
      "08a5b66845ac: Layer already exists\n",
      "88a85bcf8170: Layer already exists\n",
      "65860ac81ef4: Layer already exists\n",
      "65ef2276d16f: Layer already exists\n",
      "a22a5ac18042: Layer already exists\n",
      "6257fa9f9597: Layer already exists\n",
      "578414b395b9: Layer already exists\n",
      "abc3250a6c7f: Layer already exists\n",
      "13d5529fd232: Layer already exists\n",
      "1.0: digest: sha256:076aa3e410b5d517b0c627e6ffd033c8ec069d9a25b8351ca5c49e528a1dd282 size: 3480\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker push edwardchalstrey/classifier:1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the docker container and collect benchmark stats\n",
    "-----\n",
    "\n",
    "Here I save the results to stdout, but you could instead save the benchmark stats to a file within the container then use ```docker cp``` to move this outside the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --out docker_results\n",
    "docker run edwardchalstrey/classifier:1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_results = ast.literal_eval(docker_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do they compare?\n",
    "---\n",
    "\n",
    "In this example, the benchmark stats I have collected are the preformance stats measured by sci-kit learn, as well as the time taken to fit the classification model and the time it takes to predict the catagories of the test data.\n",
    "\n",
    "Here I have labelled the results from running the algorithm code on my machine directly as \"Basic\" and the Docker version as \"Container\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Version      </td><td>Training time (s)  </td><td>Prediction time (s)</td><td>Performance (micro avg f1 score)</td></tr>\n",
       "<tr><td>Basic 1.0    </td><td>0.10748064517974854</td><td>0.05083489418029785</td><td>0.6974416017797553              </td></tr>\n",
       "<tr><td>Container 1.0</td><td>0.14680159091949463</td><td>0.07211601734161377</td><td>0.6974416017797553              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "headers = [\"Version\"]\n",
    "c_results = [\"Basic 1.0\"]\n",
    "d_results = [\"Container 1.0\"]\n",
    "for k, v in local_results.items():\n",
    "    headers.append(k)\n",
    "    c_results.append(v)\n",
    "for k, v in docker_results.items():\n",
    "    d_results.append(v)\n",
    "display(HTML(tabulate.tabulate([headers, c_results, d_results], tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's release the next version of our algorithm\n",
    "----\n",
    "1) Navigate to Dockerhub and click the [builds tab](https://cloud.docker.com/repository/docker/edwardchalstrey/classifier/builds) for your algorithm's repository and set up [build rules](https://docs.docker.com/docker-hub/builds/) to your liking. Configure it to use the github repo where your algorithm code is mantained. In this case I have set it to simply build a new version of the container and tag as ```edwardchalstrey/classifier:latest``` whenever there is a push to the master branch of [my GitHub repo](https://github.com/edwardchalstrey1/benchmarking_test). Here I will also tag separate versions.\n",
    "\n",
    "2) Edit the classifier algorithm to create a new version (e.g. version 1.1)\n",
    "\n",
    "3) Commit and push changes to github\n",
    "\n",
    "4) The latest version can then be pulled: ```docker pull edwardchalstrey/classifier:latest```\n",
    "\n",
    "**Instead of using the automated build here I do a regular build:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile classifier.py\n",
    "from sklearn import datasets, svm, metrics\n",
    "import time\n",
    "\n",
    "def results():\n",
    "\n",
    "    digits = datasets.load_digits()\n",
    "\n",
    "    n_samples = len(digits.images)\n",
    "    data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "    expected = digits.target[n_samples // 2:]\n",
    "\n",
    "    models = [svm.SVC(gamma=0.01),\n",
    "          svm.SVC(gamma=0.001)]\n",
    "\n",
    "    ### Version 1.0 ###\n",
    "\n",
    "#     classifier = models[0]\n",
    "\n",
    "    ### Version 1.1 ###\n",
    "\n",
    "    classifier = models[1]\n",
    "\n",
    "    start = time.time()\n",
    "    classifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])\n",
    "    end = time.time()\n",
    "    training_time = end - start\n",
    "\n",
    "    start = time.time()\n",
    "    predicted = classifier.predict(data[n_samples // 2:])\n",
    "    end = time.time()\n",
    "    classifier_time = end - start\n",
    "\n",
    "    report = metrics.classification_report(expected, predicted, output_dict=True)\n",
    "\n",
    "    performance = report['micro avg']['f1-score']\n",
    "\n",
    "    return([metrics.classification_report(expected, predicted), {\"Training time (s)\": training_time, \"Prediction time (s)\": classifier_time,\n",
    "    \"Performance (micro avg f1 score)\": report['micro avg']['f1-score']}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  105.5kB\r",
      "\r\n",
      "Step 1/8 : FROM python:3\n",
      " ---> ac069ebfe1e1\n",
      "Step 2/8 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 5a84d23aa7b5\n",
      "Step 3/8 : RUN pip3 install numpy\n",
      " ---> Using cache\n",
      " ---> 4383ac463a3b\n",
      "Step 4/8 : RUN pip3 install scipy\n",
      " ---> Using cache\n",
      " ---> 6fa2c9da864b\n",
      "Step 5/8 : RUN pip3 install scikit-learn\n",
      " ---> Using cache\n",
      " ---> 0b888dbaed11\n",
      "Step 6/8 : COPY classifier.py /classifier.py\n",
      " ---> Using cache\n",
      " ---> 8ac8658c2951\n",
      "Step 7/8 : COPY iterate_benchmarks.py /iterate_benchmarks.py\n",
      " ---> Using cache\n",
      " ---> d74a3fe73239\n",
      "Step 8/8 : CMD python3 iterate_benchmarks.py\n",
      " ---> Using cache\n",
      " ---> 1b3f600ea676\n",
      "Successfully built 1b3f600ea676\n",
      "Successfully tagged edwardchalstrey/classifier:1.1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker build -t edwardchalstrey/classifier:1.1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [docker.io/edwardchalstrey/classifier]\n",
      "18661e28fe84: Preparing\n",
      "17e84f583592: Preparing\n",
      "f12d5b3c5c82: Preparing\n",
      "337d3babfd9c: Preparing\n",
      "493622b04a5f: Preparing\n",
      "65ef2276d16f: Preparing\n",
      "4b381ae03f9a: Preparing\n",
      "08a5b66845ac: Preparing\n",
      "88a85bcf8170: Preparing\n",
      "4b381ae03f9a: Waiting\n",
      "65860ac81ef4: Preparing\n",
      "a22a5ac18042: Preparing\n",
      "6257fa9f9597: Preparing\n",
      "578414b395b9: Preparing\n",
      "abc3250a6c7f: Preparing\n",
      "13d5529fd232: Preparing\n",
      "08a5b66845ac: Waiting\n",
      "88a85bcf8170: Waiting\n",
      "65860ac81ef4: Waiting\n",
      "a22a5ac18042: Waiting\n",
      "6257fa9f9597: Waiting\n",
      "578414b395b9: Waiting\n",
      "abc3250a6c7f: Waiting\n",
      "13d5529fd232: Waiting\n",
      "65ef2276d16f: Waiting\n",
      "493622b04a5f: Layer already exists\n",
      "f12d5b3c5c82: Layer already exists\n",
      "337d3babfd9c: Layer already exists\n",
      "18661e28fe84: Layer already exists\n",
      "17e84f583592: Layer already exists\n",
      "65ef2276d16f: Layer already exists\n",
      "08a5b66845ac: Layer already exists\n",
      "4b381ae03f9a: Layer already exists\n",
      "65860ac81ef4: Layer already exists\n",
      "88a85bcf8170: Layer already exists\n",
      "a22a5ac18042: Layer already exists\n",
      "6257fa9f9597: Layer already exists\n",
      "578414b395b9: Layer already exists\n",
      "13d5529fd232: Layer already exists\n",
      "abc3250a6c7f: Layer already exists\n",
      "1.1: digest: sha256:e060d94b67025d116b595492bc2fbfc7cd3164f7b4ccdc97c22993e64d58de9d size: 3480\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker push edwardchalstrey/classifier:1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then collect benchmark stats again for the new version of your algorithm and compare to previous versions and other machines/environments\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --out docker_results_1\n",
    "docker run edwardchalstrey/classifier:1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_results_1 = ast.literal_eval(docker_results_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Version      </td><td>Training time (s)  </td><td>Prediction time (s)</td><td>Performance (micro avg f1 score)</td></tr>\n",
       "<tr><td>Basic 1.0    </td><td>0.10748064517974854</td><td>0.05083489418029785</td><td>0.6974416017797553              </td></tr>\n",
       "<tr><td>Container 1.0</td><td>0.14680159091949463</td><td>0.07211601734161377</td><td>0.6974416017797553              </td></tr>\n",
       "<tr><td>Container 1.1</td><td>0.04823911190032959</td><td>0.04134511947631836</td><td>0.9688542825361512              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d1_results = [\"Container 1.1\"]\n",
    "for k, v in docker_results_1.items():\n",
    "    d1_results.append(v)\n",
    "display(HTML(tabulate.tabulate([headers, c_results, d_results, d1_results], tablefmt='html')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
